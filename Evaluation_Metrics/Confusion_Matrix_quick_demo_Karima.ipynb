{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Here is a python script which demonstrates how to create a confusion matrix on a predicted model.\n",
    "\n",
    "\n",
    "For this, we have to import confusion matrix module from sklearn library which helps us to generate the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[4 2]\n",
      " [1 3]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.70      0.71      0.70        10\n",
      "weighted avg       0.72      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "actual = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0] \n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0] \n",
    "results = confusion_matrix(actual, predicted) \n",
    "print ('Confusion Matrix :')\n",
    "print((results) )\n",
    "print ('Accuracy Score :',accuracy_score(actual, predicted) )\n",
    "print ('Report : ')\n",
    "print (classification_report(actual, predicted) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/confusion-matrix-machine-learning/http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic Data Science interview question is to ask \"What is better--more false positives, or false negatives?\" \n",
    "\n",
    "\n",
    "This is a trick question designed to test your critical thinking on the topics of precision and recall. \n",
    "\n",
    "\n",
    "\n",
    "As you're probably thinking, the answer is \"It depends on the problem!\". \n",
    "\n",
    "\n",
    "\n",
    "Sometimes, our model may be focused on a problem where False Positives are much worse than False Negatives, or vice versa. For instance, detecting credit card fraud. A False Positive would be when our model flags a transaction as fraudulent, and it isn't. This results in a slightly annoyed customer. On the other hand, a False Negative might be a fraudulent transaction that the company mistakenly lets through as normal consumer behavior. In this case, the credit card company could be on the hook for reimbursing the customer for thousands of dollars because they missed the signs that the transaction was fraudulent! Although being wrong is never ideal, it makes sense that credit card companies tend to build their models to be a bit too sensitive, because having a high recall saves them more money than having a high precision score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a few minutes and see if you can think of at least 2 examples each of situations where a high precision might be preferable to high recall, and 2 examples where high recall might be preferable to high precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Precision** says what proportion of positive predictions were correct (P = TP / (TP + FP).\n",
    "- **High precision** is preferable when we want to minimize false positives, i.e. we want to avoid tagging negatives as positives (we care more about the negatives).\n",
    "  - Example 1: Spam mail detection (where: Spam = positive, Non-Spam = negative). We want to minimize the false positive cases where the model classifies a non-spam (negative) as spam (positive). Here, it's more problematic to oversee an important mail (false positive) than to read a spam mail (false negative). \n",
    "  - Example 2: Ticket control during rush hour (where: fare dodgers = positive, paying client = negative). We want to minimze the false positive cases where the model classifies all paying persons (negative) as fare dodgers (positive). Here, it's more problematic to check every of the 500 persons in a bus during rush-hour (false positive) than to let the one fare dodger go unnoticed (false negative).\n",
    "  - Example 3: Zalando model predicting payment default (where: default = positive, paying person = negative). This model should minimize the false positive cases where the model calssifies a paying person (negative) as non-paying person (negative), e.g. it predicts payment default for ALL persons in a certain city district . Here, it's more problematic to let a paying person not order products (false positive) than to let payment defaults happen (false negative).\n",
    "\n",
    "\n",
    "- **Recall/Sensitivity** says what proportion of actual positives were identified correctly (R = TP / (TP + FN).\n",
    "- **High recall** is preferable when we want to minimize false negatives, i.e. we want to avoid tagging positives as negative (we care more about the positives).\n",
    "  - Example 1. Airport control tools, e.g. video analysis of movements (where: Terrorist = positive, Non-Terrorist = negative). We want to minimize the false negative cases where the model classifies a terrorist (positive) as harmless (negative). Here, it's less problematic to control a harmless person (false positive) than to oversee a terrorist (false negative).\n",
    "  - Example 2: Medical newborn control checkups (where: disease = positive, healthy = negative). We want to minimize the false negative cases where the model classifies a sick child (positive) as healthy (negative). Here, it's less problematic to check a healthy newborn (false positive) than to oversee and not treat a sick child (false negative).\n",
    "  - Example 3: Model predicting re-hospitalization (where: re-hospitalized = positive, non-rehospitalized = negative). We want to minimize the false negative cases where the model classifies a person being in risk for re-hospitalization (positive) as non-risky (negative). Here, it's less problematic to give special care to a person that is actually not in risk for re-hospitalization (false positive) than to oversee a person in risk of re-hospitalization for which special care might prevent dealth or re-hospitalization (false negative)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
